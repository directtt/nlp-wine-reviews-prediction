{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c3d167",
   "metadata": {},
   "source": [
    "# 03-wine_reviews_transformers_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae017807",
   "metadata": {},
   "source": [
    "# 1) Fine-tuning DistilBERT\n",
    "\n",
    "Fine-tuning [DistilBERT](https://huggingface.co/distilbert-base-uncased) model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "143e7994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:25.302863Z",
     "start_time": "2023-05-10T17:03:25.088536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d8bbf",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa270334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:26.217380Z",
     "start_time": "2023-05-10T17:03:25.304756Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wine-reviews.csv')\n",
    "df = df[['description', 'points']]\n",
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c1a2a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:26.453206Z",
     "start_time": "2023-05-10T17:03:26.219383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97826</th>\n",
       "      <td>A Syrah-Grenache blend that's dry and rustical...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97827</th>\n",
       "      <td>Oreo eaters will enjoy the aromas of this wine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97828</th>\n",
       "      <td>Outside of the vineyard, wines like this are w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97829</th>\n",
       "      <td>Heavy and basic, with melon and pineapple arom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97830</th>\n",
       "      <td>Smooth in the mouth, this Chard starts off wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description  points\n",
       "0      This tremendous 100% varietal wine hails from ...       4\n",
       "1      Ripe aromas of fig, blackberry and cassis are ...       4\n",
       "2      Mac Watson honors the memory of a wine once ma...       4\n",
       "3      This spent 20 months in 30% new French oak, an...       4\n",
       "4      This is the top wine from La Bégude, named aft...       4\n",
       "...                                                  ...     ...\n",
       "97826  A Syrah-Grenache blend that's dry and rustical...       1\n",
       "97827  Oreo eaters will enjoy the aromas of this wine...       1\n",
       "97828  Outside of the vineyard, wines like this are w...       1\n",
       "97829  Heavy and basic, with melon and pineapple arom...       1\n",
       "97830  Smooth in the mouth, this Chard starts off wit...       1\n",
       "\n",
       "[97831 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def points_binning(points):\n",
    "    if points >= 80 and points <= 84:\n",
    "        return 1\n",
    "    elif points >= 85 and points <= 89:\n",
    "        return 2 \n",
    "    elif points >= 90 and points <= 94:\n",
    "        return 3 \n",
    "    elif points >= 95 and points <= 100:\n",
    "        return 4\n",
    "    \n",
    "df['points'] = df['points'].apply(points_binning)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bc01a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:27.114352Z",
     "start_time": "2023-05-10T17:03:26.455109Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/wine-reviews-preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbfedebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:27.349643Z",
     "start_time": "2023-05-10T17:03:27.116223Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'description': 'text', 'points': 'label'})\n",
    "X = df['text']\n",
    "y = df['label'] - 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8744d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T17:03:27.553662Z",
     "start_time": "2023-05-10T17:03:27.351575Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431067a",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111f2846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:10:47.034825Z",
     "start_time": "2023-05-10T13:10:44.105834Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea9faaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T13:10:47.114696Z",
     "start_time": "2023-05-10T13:10:47.034825Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf8721b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:25:15.128987Z",
     "start_time": "2023-05-10T11:24:58.295005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dd42116fda4c56b9d7f4a4c53e666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfda0042d2f493ab21057ad8635b6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_test = test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7074a33",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eafea2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:58:54.807024Z",
     "start_time": "2023-05-10T08:58:43.306648Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca97b008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:40:00.012031Z",
     "start_time": "2023-05-10T08:39:59.735600Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "399cdfdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:22:50.737329Z",
     "start_time": "2023-05-09T08:22:48.247226Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ad5ee31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:58:58.404318Z",
     "start_time": "2023-05-10T08:58:57.636692Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_test,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62e73ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:22:50.892972Z",
     "start_time": "2023-05-09T08:22:50.882972Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "86c0d8f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T08:39:58.036883Z",
     "start_time": "2023-05-09T08:39:58.016879Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)\n",
    "# Trained on virutal machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da782c1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984ad28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:58:57.629860Z",
     "start_time": "2023-05-10T08:58:54.812027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at directtt/wine-reviews-distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"directtt/wine-reviews-distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc85b142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T09:06:02.080572Z",
     "start_time": "2023-05-10T09:06:01.795100Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "685cd902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T10:51:05.574973Z",
     "start_time": "2023-05-09T10:51:05.539468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.72      2924\n",
      "           1       0.78      0.82      0.80     10187\n",
      "           2       0.77      0.76      0.76      6074\n",
      "           3       0.76      0.33      0.46       382\n",
      "\n",
      "    accuracy                           0.77     19567\n",
      "   macro avg       0.76      0.65      0.69     19567\n",
      "weighted avg       0.77      0.77      0.77     19567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(pred.logits, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3da746",
   "metadata": {},
   "source": [
    "+0.03 boost from against LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cf60a",
   "metadata": {},
   "source": [
    "## Evaluation from pipeline (this takes a bit more time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3834c20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T11:46:48.554333Z",
     "start_time": "2023-05-09T11:46:39.202358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at directtt/wine-reviews-distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = pipeline(\"text-classification\",\n",
    "                 model=\"directtt/wine-reviews-distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27daa88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T12:34:59.321293Z",
     "start_time": "2023-05-09T11:47:04.562679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19567/19567 [47:54<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for text in tqdm(X_test):\n",
    "    y_pred.append(int(model(text)[0]['label'][-1])) # parsing from 'label_X' to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81455ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T13:00:12.455083Z",
     "start_time": "2023-05-09T13:00:12.414919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.72      2924\n",
      "           1       0.78      0.82      0.80     10187\n",
      "           2       0.77      0.76      0.76      6074\n",
      "           3       0.76      0.33      0.46       382\n",
      "\n",
      "    accuracy                           0.77     19567\n",
      "   macro avg       0.76      0.65      0.69     19567\n",
      "weighted avg       0.77      0.77      0.77     19567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf9b50",
   "metadata": {},
   "source": [
    "# 2) Fine-tuning RoBERTa\n",
    "\n",
    "Fine-tuning [RoBERTa](https://huggingface.co/xlm-roberta-base) model from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3de356",
   "metadata": {},
   "source": [
    "Preprocessing & tokenizing & training steps are same as before, model has been trained on virtual machine. <br>\n",
    "Transforming from raw pandas dataframe to transformer ready tensorflow dataset has been defined inside external function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a84301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T16:47:15.867170Z",
     "start_time": "2023-05-10T16:47:10.169962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at directtt/wine-reviews-roberta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"directtt/wine-reviews-roberta\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"directtt/wine-reviews-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9393d213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T16:47:18.839631Z",
     "start_time": "2023-05-10T16:47:15.870162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3113ab5097436ba0d3757b5e96426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from scripts.transformers.df_to_tf_dataset import df_to_tf_dataset\n",
    "\n",
    "tf_test_set = df_to_tf_dataset(df=test, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab7dcf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa60d5ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:58:39.998334Z",
     "start_time": "2023-05-10T11:25:55.028825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1965s 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48538c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T11:58:40.407575Z",
     "start_time": "2023-05-10T11:58:40.001193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73      2924\n",
      "           1       0.79      0.81      0.80     10187\n",
      "           2       0.77      0.74      0.75      6074\n",
      "           3       0.54      0.54      0.54       382\n",
      "\n",
      "    accuracy                           0.77     19567\n",
      "   macro avg       0.71      0.70      0.71     19567\n",
      "weighted avg       0.77      0.77      0.77     19567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(y_pred.logits, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27ef66",
   "metadata": {},
   "source": [
    "# 3) Fine-tuning GPT-2\n",
    "\n",
    "Fine-tuning [GPT-2](https://huggingface.co/gpt2) model from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b880c",
   "metadata": {},
   "source": [
    "Preprocessing & tokenizing & training steps are same as before, model has been trained on virtual machine. <br>\n",
    "Transforming from raw pandas dataframe to transformer ready tensorflow dataset has been defined inside external function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a7060",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ac55ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T16:56:25.975820Z",
     "start_time": "2023-05-10T16:56:20.230685Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at directtt/wine-reviews-distilbert were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at directtt/wine-reviews-distilbert and are newly initialized: ['dropout_151']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = pipeline(\"text-classification\",\n",
    "                 model=\"directtt/wine-reviews-distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b04ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T16:40:04.081964Z",
     "start_time": "2023-05-10T15:56:16.047827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19567/19567 [43:46<00:00,  7.45it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for text in tqdm(X_test):\n",
    "    y_pred.append(int(model(text)[0]['label'][-1])) # parsing from 'label_X' to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9e9358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T16:40:11.469337Z",
     "start_time": "2023-05-10T16:40:11.224340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.72      2924\n",
      "           1       0.78      0.82      0.80     10187\n",
      "           2       0.77      0.76      0.76      6074\n",
      "           3       0.76      0.33      0.46       382\n",
      "\n",
      "    accuracy                           0.77     19567\n",
      "   macro avg       0.76      0.65      0.69     19567\n",
      "weighted avg       0.77      0.77      0.77     19567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
